{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install torch_xla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100  3727  100  3727    0     0  17747      0 --:--:-- --:--:-- --:--:-- 17747\r\n",
      "Updating TPU and VM. This may take around 2 minutes.\r\n",
      "Updating TPU runtime to pytorch-dev20200325 ...\r\n",
      "Found existing installation: torch 1.4.0\r\n",
      "Uninstalling torch-1.4.0:\r\n",
      "Done updating TPU runtime: <Response [200]>\r\n",
      "  Successfully uninstalled torch-1.4.0\r\n",
      "Found existing installation: torchvision 0.5.0\r\n",
      "Uninstalling torchvision-0.5.0:\r\n",
      "  Successfully uninstalled torchvision-0.5.0\r\n",
      "Copying gs://tpu-pytorch/wheels/torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/83.4 MiB.                                     \r\n",
      "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/114.5 MiB.                                    \r\n",
      "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/2.5 MiB.                                      \r\n",
      "Processing ./torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch==nightly+20200325) (0.18.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch==nightly+20200325) (1.18.2)\r\n",
      "\u001b[31mERROR: fastai 1.0.60 requires torchvision, which is not installed.\u001b[0m\r\n",
      "\u001b[31mERROR: catalyst 20.3.3 requires torchvision>=0.2.1, which is not installed.\u001b[0m\r\n",
      "\u001b[31mERROR: allennlp 0.9.0 has requirement spacy<2.2,>=2.1.0, but you'll have spacy 2.2.3 which is incompatible.\u001b[0m\r\n",
      "Installing collected packages: torch\r\n",
      "Successfully installed torch-1.5.0a0+d6149a7\r\n",
      "Processing ./torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl\r\n",
      "Installing collected packages: torch-xla\r\n",
      "Successfully installed torch-xla-1.6+e788e5b\r\n",
      "Processing ./torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from torchvision==nightly+20200325) (1.5.0a0+d6149a7)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision==nightly+20200325) (5.4.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchvision==nightly+20200325) (1.14.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torchvision==nightly+20200325) (1.18.2)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch->torchvision==nightly+20200325) (0.18.2)\r\n",
      "Installing collected packages: torchvision\r\n",
      "Successfully installed torchvision-0.6.0a0+3c254fb\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "The following additional packages will be installed:\r\n",
      "  gfortran gfortran-6 libblas-common libblas-dev libblas3 libgfortran-6-dev\r\n",
      "  libgfortran3 libopenblas-base\r\n",
      "Suggested packages:\r\n",
      "  gfortran-multilib gfortran-doc gfortran-6-multilib gfortran-6-doc\r\n",
      "  libgfortran3-dbg libcoarrays-dev liblapack-doc-man liblapack-doc\r\n",
      "The following NEW packages will be installed:\r\n",
      "  gfortran gfortran-6 libblas-common libblas-dev libblas3 libgfortran-6-dev\r\n",
      "  libgfortran3 libomp5 libopenblas-base libopenblas-dev\r\n",
      "0 upgraded, 10 newly installed, 0 to remove and 37 not upgraded.\r\n",
      "Need to get 15.6 MB of archives.\r\n",
      "After this operation, 122 MB of additional disk space will be used.\r\n",
      "Get:1 http://deb.debian.org/debian stretch/main amd64 libgfortran3 amd64 6.3.0-18+deb9u1 [265 kB]\r\n",
      "Get:2 http://deb.debian.org/debian stretch/main amd64 libgfortran-6-dev amd64 6.3.0-18+deb9u1 [299 kB]\r\n",
      "Get:3 http://deb.debian.org/debian stretch/main amd64 gfortran-6 amd64 6.3.0-18+deb9u1 [6916 kB]\r\n",
      "Get:4 http://deb.debian.org/debian stretch/main amd64 gfortran amd64 4:6.3.0-4 [1356 B]\r\n",
      "Get:5 http://deb.debian.org/debian stretch/main amd64 libblas-common amd64 3.7.0-2 [14.2 kB]\r\n",
      "Get:6 http://deb.debian.org/debian stretch/main amd64 libblas3 amd64 3.7.0-2 [155 kB]\r\n",
      "Get:7 http://deb.debian.org/debian stretch/main amd64 libblas-dev amd64 3.7.0-2 [162 kB]\r\n",
      "Get:8 http://deb.debian.org/debian stretch/main amd64 libopenblas-base amd64 0.2.19-3 [3793 kB]\r\n",
      "Get:9 http://deb.debian.org/debian stretch/main amd64 libopenblas-dev amd64 0.2.19-3 [3809 kB]\r\n",
      "Get:10 http://deb.debian.org/debian stretch/main amd64 libomp5 amd64 3.9.1-1 [228 kB]\r\n",
      "Fetched 15.6 MB in 0s (66.7 MB/s)\r\n",
      "debconf: delaying package configuration, since apt-utils is not installed\r\n",
      "Selecting previously unselected package libgfortran3:amd64.\r\n",
      "(Reading database ... 72030 files and directories currently installed.)\r\n",
      "Preparing to unpack .../0-libgfortran3_6.3.0-18+deb9u1_amd64.deb ...\r\n",
      "Unpacking libgfortran3:amd64 (6.3.0-18+deb9u1) ...\r\n",
      "Selecting previously unselected package libgfortran-6-dev:amd64.\r\n",
      "Preparing to unpack .../1-libgfortran-6-dev_6.3.0-18+deb9u1_amd64.deb ...\r\n",
      "Unpacking libgfortran-6-dev:amd64 (6.3.0-18+deb9u1) ...\r\n",
      "Selecting previously unselected package gfortran-6.\r\n",
      "Preparing to unpack .../2-gfortran-6_6.3.0-18+deb9u1_amd64.deb ...\r\n",
      "Unpacking gfortran-6 (6.3.0-18+deb9u1) ...\r\n",
      "Selecting previously unselected package gfortran.\r\n",
      "Preparing to unpack .../3-gfortran_4%3a6.3.0-4_amd64.deb ...\r\n",
      "Unpacking gfortran (4:6.3.0-4) ...\r\n",
      "Selecting previously unselected package libblas-common.\r\n",
      "Preparing to unpack .../4-libblas-common_3.7.0-2_amd64.deb ...\r\n",
      "Unpacking libblas-common (3.7.0-2) ...\r\n",
      "Selecting previously unselected package libblas3.\r\n",
      "Preparing to unpack .../5-libblas3_3.7.0-2_amd64.deb ...\r\n",
      "Unpacking libblas3 (3.7.0-2) ...\r\n",
      "Selecting previously unselected package libblas-dev.\r\n",
      "Preparing to unpack .../6-libblas-dev_3.7.0-2_amd64.deb ...\r\n",
      "Unpacking libblas-dev (3.7.0-2) ...\r\n",
      "Selecting previously unselected package libopenblas-base.\r\n",
      "Preparing to unpack .../7-libopenblas-base_0.2.19-3_amd64.deb ...\r\n",
      "Unpacking libopenblas-base (0.2.19-3) ...\r\n",
      "Selecting previously unselected package libopenblas-dev.\r\n",
      "Preparing to unpack .../8-libopenblas-dev_0.2.19-3_amd64.deb ...\r\n",
      "Unpacking libopenblas-dev (0.2.19-3) ...\r\n",
      "Selecting previously unselected package libomp5:amd64.\r\n",
      "Preparing to unpack .../9-libomp5_3.9.1-1_amd64.deb ...\r\n",
      "Unpacking libomp5:amd64 (3.9.1-1) ...\r\n",
      "Setting up libomp5:amd64 (3.9.1-1) ...\r\n",
      "Setting up libblas-common (3.7.0-2) ...\r\n",
      "Setting up libgfortran3:amd64 (6.3.0-18+deb9u1) ...\r\n",
      "Setting up libgfortran-6-dev:amd64 (6.3.0-18+deb9u1) ...\r\n",
      "Setting up libblas3 (3.7.0-2) ...\r\n",
      "update-alternatives: using /usr/lib/libblas/libblas.so.3 to provide /usr/lib/libblas.so.3 (libblas.so.3) in auto mode\r\n",
      "Processing triggers for libc-bin (2.24-11+deb9u4) ...\r\n",
      "Setting up libopenblas-base (0.2.19-3) ...\r\n",
      "update-alternatives: using /usr/lib/openblas-base/libblas.so.3 to provide /usr/lib/libblas.so.3 (libblas.so.3) in auto mode\r\n",
      "update-alternatives: using /usr/lib/openblas-base/liblapack.so.3 to provide /usr/lib/liblapack.so.3 (liblapack.so.3) in auto mode\r\n",
      "Setting up gfortran-6 (6.3.0-18+deb9u1) ...\r\n",
      "Setting up gfortran (4:6.3.0-4) ...\r\n",
      "update-alternatives: using /usr/bin/gfortran to provide /usr/bin/f95 (f95) in auto mode\r\n",
      "update-alternatives: using /usr/bin/gfortran to provide /usr/bin/f77 (f77) in auto mode\r\n",
      "Setting up libblas-dev (3.7.0-2) ...\r\n",
      "update-alternatives: using /usr/lib/libblas/libblas.so to provide /usr/lib/libblas.so (libblas.so) in auto mode\r\n",
      "Setting up libopenblas-dev (0.2.19-3) ...\r\n",
      "update-alternatives: using /usr/lib/openblas-base/libblas.so to provide /usr/lib/libblas.so (libblas.so) in auto mode\r\n",
      "update-alternatives: using /usr/lib/openblas-base/liblapack.so to provide /usr/lib/liblapack.so (liblapack.so) in auto mode\r\n",
      "Processing triggers for libc-bin (2.24-11+deb9u4) ...\r\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and utility scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dataset\n",
    "import engine\n",
    "import torch\n",
    "import transformers\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from model import JigsawModel\n",
    "\n",
    "from sklearn import metrics\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    MAX_LEN = 192\n",
    "    TRAIN_BATCH_SIZE = 64\n",
    "    VALID_BATCH_SIZE = 4\n",
    "    EPOCHS = 1\n",
    "    LEARNING_RATE = 0.5e-5\n",
    "    BERT_PATH = \"../input/bert-base-multilingual-uncased/\"\n",
    "    MODEL_PATH = \"model.bin\"\n",
    "    TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
    "        BERT_PATH,\n",
    "        do_lower_case=True\n",
    "    )\n",
    "    JIGSAW_DATA_PATH = \"../input/jigsaw-multilingual-toxic-comment-classification/\"\n",
    "    TRAINING_FILE_1 = os.path.join(\n",
    "        JIGSAW_DATA_PATH, \n",
    "        \"jigsaw-toxic-comment-train.csv\"\n",
    "    )\n",
    "    TRAINING_FILE_2 = os.path.join(\n",
    "        JIGSAW_DATA_PATH, \n",
    "        \"jigsaw-unintended-bias-train.csv\"\n",
    "    )\n",
    "    VALIDATION_FILE = os.path.join(\n",
    "        JIGSAW_DATA_PATH, \n",
    "        \"validation.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MX = JigsawModel(config.BERT_PATH)\n",
    "\n",
    "df_train1 = pd.read_csv(\n",
    "    config.TRAINING_FILE_1, \n",
    "    usecols=[\"comment_text\", \"toxic\"]\n",
    ").fillna(\"none\")\n",
    "\n",
    "df_train2 = pd.read_csv(\n",
    "    config.TRAINING_FILE_2, \n",
    "    usecols=[\"comment_text\", \"toxic\"]\n",
    ").fillna(\"none\")\n",
    "\n",
    "df_valid = pd.read_csv(config.VALIDATION_FILE)\n",
    "\n",
    "df_train = pd.concat([df_train1, df_train2], axis=0).reset_index(drop=True)\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True).head(200000)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_valid = df_valid.reset_index(drop=True)\n",
    "\n",
    "train_targets = df_train.toxic.values\n",
    "valid_targets = df_valid.toxic.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    train_dataset = dataset.JigsawTraining(\n",
    "        comment_text=df_train.comment_text.values,\n",
    "        targets=train_targets,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "          train_dataset,\n",
    "          num_replicas=xm.xrt_world_size(),\n",
    "          rank=xm.get_ordinal(),\n",
    "          shuffle=True)\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.TRAIN_BATCH_SIZE,\n",
    "        sampler=train_sampler,\n",
    "        drop_last=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    valid_dataset = dataset.JigsawTraining(\n",
    "        comment_text=df_valid.comment_text.values,\n",
    "        targets=valid_targets,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "          valid_dataset,\n",
    "          num_replicas=xm.xrt_world_size(),\n",
    "          rank=xm.get_ordinal(),\n",
    "          shuffle=False)\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config.VALID_BATCH_SIZE,\n",
    "        sampler=valid_sampler,\n",
    "        drop_last=False,\n",
    "        num_workers=1\n",
    "    )\n",
    "\n",
    "    device = xm.xla_device()\n",
    "    model = MX.to(device)\n",
    "    \n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            'params': [\n",
    "                p for n, p in param_optimizer if not any(\n",
    "                    nd in n for nd in no_decay\n",
    "                )\n",
    "            ], \n",
    "            'weight_decay': 0.001\n",
    "        },\n",
    "        {\n",
    "            'params': [\n",
    "                p for n, p in param_optimizer if any(\n",
    "                    nd in n for nd in no_decay\n",
    "                )\n",
    "            ],\n",
    "            'weight_decay': 0.0\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    num_train_steps = int(\n",
    "        len(df_train) / config.TRAIN_BATCH_SIZE / xm.xrt_world_size() * config.EPOCHS\n",
    "    )\n",
    "    optimizer = AdamW(\n",
    "        optimizer_parameters, \n",
    "        lr=config.LEARNING_RATE * xm.xrt_world_size()\n",
    "    )\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    best_auc = 0\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        para_loader = pl.ParallelLoader(train_data_loader, [device])\n",
    "        engine.train_fn(\n",
    "            para_loader.per_device_loader(device), \n",
    "            model, \n",
    "            optimizer, \n",
    "            device, \n",
    "            scheduler\n",
    "        )\n",
    "        \n",
    "        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n",
    "        outputs, targets = engine.eval_fn(\n",
    "            para_loader.per_device_loader(device), \n",
    "            model, \n",
    "            device\n",
    "        )\n",
    "\n",
    "        targets = np.array(targets) >= 0.5\n",
    "        auc = metrics.roc_auc_score(targets, outputs)\n",
    "        print(f'[xla:{xm.get_ordinal()}]: AUC={auc}')\n",
    "        if auc > best_auc:\n",
    "            xm.save(model.state_dict(), config.MODEL_PATH)\n",
    "            best_auc = auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-processing wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mp_fn(rank, flags):\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    a = run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process spawner for training on TPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[xla:7]: bi=0, loss=0.7013534903526306\n",
      "[xla:5]: bi=0, loss=0.710527777671814\n",
      "[xla:3]: bi=0, loss=0.7302920818328857\n",
      "[xla:6]: bi=0, loss=0.7405128479003906\n",
      "[xla:4]: bi=0, loss=0.759679913520813\n",
      "[xla:1]: bi=0, loss=0.7210890650749207\n",
      "[xla:0]: bi=0, loss=0.7046725749969482\n",
      "[xla:2]: bi=0, loss=0.70517897605896\n",
      "[xla:4]: bi=100, loss=0.2059505432844162\n",
      "[xla:2]: bi=100, loss=0.25164592266082764\n",
      "[xla:3]: bi=100, loss=0.28767266869544983\n",
      "[xla:7]: bi=100, loss=0.24973106384277344\n",
      "[xla:5]: bi=100, loss=0.24740687012672424\n",
      "[xla:1]: bi=100, loss=0.22373899817466736\n",
      "[xla:6]: bi=100, loss=0.19031786918640137\n",
      "[xla:0]: bi=100, loss=0.221267968416214\n",
      "[xla:6]: bi=200, loss=0.21090705692768097\n",
      "[xla:1]: bi=200, loss=0.18781417608261108\n",
      "[xla:4]: bi=200, loss=0.1527944803237915\n",
      "[xla:3]: bi=200, loss=0.20945271849632263\n",
      "[xla:7]: bi=200, loss=0.20232856273651123\n",
      "[xla:2]: bi=200, loss=0.22009186446666718\n",
      "[xla:0]: bi=200, loss=0.2254747748374939\n",
      "[xla:5]: bi=200, loss=0.24092696607112885\n",
      "[xla:3]: bi=300, loss=0.1984182447195053\n",
      "[xla:7]: bi=300, loss=0.23061725497245789\n",
      "[xla:6]: bi=300, loss=0.15459689497947693\n",
      "[xla:0]: bi=300, loss=0.2188001424074173\n",
      "[xla:4]: bi=300, loss=0.2514168918132782\n",
      "[xla:5]: bi=300, loss=0.25353893637657166\n",
      "[xla:1]: bi=300, loss=0.2381606101989746\n",
      "[xla:2]: bi=300, loss=0.2937258780002594\n",
      "[xla:5]: AUC=0.8452426356763063\n",
      "[xla:2]: AUC=0.8482193990723935\n",
      "[xla:3]: AUC=0.8343619649017754\n",
      "[xla:4]: AUC=0.8613804780438463\n",
      "[xla:0]: AUC=0.8546615394600207\n",
      "[xla:1]: AUC=0.8578527829864728\n",
      "[xla:7]: AUC=0.8394595684613834\n",
      "[xla:6]: AUC=0.8610180898913293\n"
     ]
    }
   ],
   "source": [
    "FLAGS={}\n",
    "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
