{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BILSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O40nvjbI5pLs",
        "colab_type": "code",
        "outputId": "48702641-455a-4325-db16-6a752a1fcd27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYiiNYWwCS3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import os\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdJI1mP4CTG0",
        "colab_type": "code",
        "outputId": "3b27430f-55e6-44d4-9151-2bfca21ce1de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/BERT/jigsaw-multilingual-toxic-comment-classification/\n",
        "train = pd.read_csv(\"data/jigsaw-toxic-comment-train.csv\")\n",
        "test = pd.read_csv(\"data/test.csv\")\n",
        "print(\"Train shape : \", train.shape)\n",
        "print(\"Test shape : \", test.shape)\n",
        "train_df, val_df = train_test_split(train, test_size=0.1)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/BERT/jigsaw-multilingual-toxic-comment-classification\n",
            "Train shape :  (223549, 8)\n",
            "Test shape :  (63812, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VHhEUNWCy-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_size = 300 # how big is each word vector\n",
        "max_features = None # how many unique words to use (i.e num rows in embedding vector)\n",
        "maxlen = 72 # max number of words in a question to use #99.99%\n",
        "\n",
        "## fill up the missing values\n",
        "X = train[\"comment_text\"].fillna(\"_na_\").values\n",
        "X_test = test[\"content\"].fillna(\"_na_\").values\n",
        "\n",
        "## Tokenize the sentences\n",
        "tokenizer = Tokenizer(num_words=max_features, filters='')\n",
        "tokenizer.fit_on_texts(list(X))\n",
        "\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "## Pad the sentences \n",
        "X = pad_sequences(X, maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "\n",
        "## Get the target values\n",
        "Y = train['toxic'].values\n",
        "\n",
        "sub = test[['id']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwBjso4tCTSQ",
        "colab_type": "code",
        "outputId": "6d943504-e200-4bab-c422-70ea7853ba3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open('quora/glove.840B.300d/glove.840B.300d.txt')\n",
        "for line in tqdm(f):\n",
        "    values = line.split(\" \")\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "560099it [00:50, 13004.92it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty3AwyU-CTUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_to_array(text):\n",
        "    empyt_emb = np.zeros(300)\n",
        "    text = text[:-1].split()[:30]\n",
        "    embeds = [embeddings_index.get(x, empyt_emb) for x in text]\n",
        "    embeds+= [empyt_emb] * (30 - len(embeds))\n",
        "    return np.array(embeds)\n",
        "\n",
        "# train_vects = [text_to_array(X_text) for X_text in tqdm(train_df[\"question_text\"])]\n",
        "val_vects = np.array([text_to_array(X_text) for X_text in tqdm(val_df[\"comment_text\"][:3000])])\n",
        "val_y = np.array(val_df[\"toxic\"][:3000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYcdiX-OCTXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "def batch_gen(train_df):\n",
        "    n_batches = math.ceil(len(train_df) / batch_size)\n",
        "    while True: \n",
        "        train_df = train_df.sample(frac=1.)  # Shuffle the data.\n",
        "        for i in range(n_batches):\n",
        "            texts = train_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
        "            text_arr = np.array([text_to_array(text) for text in texts])\n",
        "            yield text_arr, np.array(train_df[\"toxic\"][i*batch_size:(i+1)*batch_size])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-1zGptECTZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Bidirectional"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzuQV41yCTch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True),\n",
        "                        input_shape=(30, 300)))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIdrkWNvGagk",
        "colab_type": "code",
        "outputId": "be699ef6-1016-4af8-b190-8c843c5b1a90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        }
      },
      "source": [
        "mg = batch_gen(train_df)\n",
        "model.fit_generator(mg, epochs=20,\n",
        "                    steps_per_epoch=1000,\n",
        "                    validation_data=(val_vects, val_y),\n",
        "                    verbose=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1000/1000 [==============================] - 213s 213ms/step - loss: 0.1611 - accuracy: 0.9417 - val_loss: 0.1470 - val_accuracy: 0.9453\n",
            "Epoch 2/20\n",
            "1000/1000 [==============================] - 211s 211ms/step - loss: 0.1412 - accuracy: 0.9493 - val_loss: 0.1448 - val_accuracy: 0.9440\n",
            "Epoch 3/20\n",
            "1000/1000 [==============================] - 212s 212ms/step - loss: 0.1372 - accuracy: 0.9502 - val_loss: 0.1397 - val_accuracy: 0.9480\n",
            "Epoch 4/20\n",
            "1000/1000 [==============================] - 210s 210ms/step - loss: 0.1296 - accuracy: 0.9531 - val_loss: 0.1400 - val_accuracy: 0.9480\n",
            "Epoch 5/20\n",
            "1000/1000 [==============================] - 214s 214ms/step - loss: 0.1271 - accuracy: 0.9538 - val_loss: 0.1495 - val_accuracy: 0.9400\n",
            "Epoch 6/20\n",
            "1000/1000 [==============================] - 213s 213ms/step - loss: 0.1195 - accuracy: 0.9566 - val_loss: 0.1385 - val_accuracy: 0.9480\n",
            "Epoch 7/20\n",
            "1000/1000 [==============================] - 209s 209ms/step - loss: 0.1106 - accuracy: 0.9595 - val_loss: 0.1433 - val_accuracy: 0.9427\n",
            "Epoch 8/20\n",
            "1000/1000 [==============================] - 208s 208ms/step - loss: 0.1063 - accuracy: 0.9612 - val_loss: 0.1621 - val_accuracy: 0.9437\n",
            "Epoch 9/20\n",
            "1000/1000 [==============================] - 211s 211ms/step - loss: 0.0953 - accuracy: 0.9652 - val_loss: 0.1589 - val_accuracy: 0.9400\n",
            "Epoch 10/20\n",
            "1000/1000 [==============================] - 213s 213ms/step - loss: 0.0850 - accuracy: 0.9692 - val_loss: 0.1725 - val_accuracy: 0.9403\n",
            "Epoch 11/20\n",
            "1000/1000 [==============================] - 214s 214ms/step - loss: 0.0806 - accuracy: 0.9707 - val_loss: 0.1806 - val_accuracy: 0.9400\n",
            "Epoch 12/20\n",
            "1000/1000 [==============================] - 211s 211ms/step - loss: 0.0637 - accuracy: 0.9775 - val_loss: 0.2002 - val_accuracy: 0.9430\n",
            "Epoch 13/20\n",
            "1000/1000 [==============================] - 212s 212ms/step - loss: 0.0594 - accuracy: 0.9789 - val_loss: 0.2231 - val_accuracy: 0.9430\n",
            "Epoch 14/20\n",
            "1000/1000 [==============================] - 216s 216ms/step - loss: 0.0529 - accuracy: 0.9808 - val_loss: 0.2318 - val_accuracy: 0.9387\n",
            "Epoch 15/20\n",
            "1000/1000 [==============================] - 212s 212ms/step - loss: 0.0418 - accuracy: 0.9853 - val_loss: 0.2640 - val_accuracy: 0.9363\n",
            "Epoch 16/20\n",
            "1000/1000 [==============================] - 214s 214ms/step - loss: 0.0402 - accuracy: 0.9858 - val_loss: 0.2848 - val_accuracy: 0.9360\n",
            "Epoch 17/20\n",
            "1000/1000 [==============================] - 211s 211ms/step - loss: 0.0334 - accuracy: 0.9884 - val_loss: 0.2804 - val_accuracy: 0.9373\n",
            "Epoch 18/20\n",
            "1000/1000 [==============================] - 213s 213ms/step - loss: 0.0293 - accuracy: 0.9899 - val_loss: 0.3085 - val_accuracy: 0.9320\n",
            "Epoch 19/20\n",
            "1000/1000 [==============================] - 212s 212ms/step - loss: 0.0277 - accuracy: 0.9904 - val_loss: 0.3337 - val_accuracy: 0.9403\n",
            "Epoch 20/20\n",
            "1000/1000 [==============================] - 215s 215ms/step - loss: 0.0206 - accuracy: 0.9933 - val_loss: 0.3510 - val_accuracy: 0.9370\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fec4acfceb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6GblbvWGajo",
        "colab_type": "code",
        "outputId": "dee4ca99-6a20-4714-b089-b4e601648f87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "batch_size = 256\n",
        "def batch_gen(test_df):\n",
        "    n_batches = math.ceil(len(test_df) / batch_size)\n",
        "    for i in range(n_batches):\n",
        "        texts = test_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
        "        text_arr = np.array([text_to_array(text) for text in texts])\n",
        "        yield text_arr\n",
        "\n",
        "test_df = test\n",
        "\n",
        "all_preds = []\n",
        "for x in tqdm(batch_gen(test_df)):\n",
        "    all_preds.extend(model.predict(x).flatten())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "250it [00:40,  6.15it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37U2_iSmGamf",
        "colab_type": "code",
        "outputId": "a8b4854a-4b37-4ad5-8e09-718db09cb534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        }
      },
      "source": [
        "y_te = (np.array(all_preds) > 0.5).astype(np.int)\n",
        "submit_df = pd.DataFrame({\"id\": test_df[\"id\"], \"toxic\": y_te})\n",
        "submit_df.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'qid'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7426ec63d038>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0my_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msubmit_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"qid\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"qid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_te\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msubmit_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"submission.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'qid'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f5L4vxnGapH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('models/bilstm.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AxVCHLxGarr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l53muNGGGauc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQLi1hVZCTfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilyCddsiCTh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDFVHkTM5rZF",
        "colab_type": "code",
        "outputId": "6777969d-c11e-44c8-cf4a-b0deb2204892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/BERT/jigsaw-multilingual-toxic-comment-classification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVNnnlc55dRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn import metrics\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras.initializers import *\n",
        "from keras.optimizers import *\n",
        "import keras.backend as K\n",
        "from keras.callbacks import *\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time\n",
        "import gc\n",
        "import re\n",
        "# from unidecode import unidecode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4JIEsLu5eJ9",
        "colab_type": "code",
        "outputId": "d3c586bf-3076-4183-f833-6be2f8fe4355",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape :  (223549, 8)\n",
            "Test shape :  (63812, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gTec3bk5enH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train[\"comment_text\"] = train[\"comment_text\"].str.lower()\n",
        "test[\"content\"] = test[\"content\"].str.lower()\n",
        "\n",
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
        "def clean_text(x):\n",
        "\n",
        "    x = str(x)\n",
        "    for punct in puncts:\n",
        "        x = x.replace(punct, f' {punct} ')\n",
        "    return x\n",
        "\n",
        "\n",
        "train[\"comment_text\"] = train[\"comment_text\"].apply(lambda x: clean_text(x))\n",
        "test[\"content\"] = test[\"content\"].apply(lambda x: clean_text(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doUf1sMr5err",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.drop(['severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)\n",
        "xtrain, xvalid, ytrain, yvalid = train_test_split(train.comment_text.values, train.toxic.values, \n",
        "                                                  stratify=train.toxic.values, \n",
        "                                                  random_state=42, \n",
        "                                                  test_size=0.2, shuffle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV1PZjfm5eux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_size = 300 # how big is each word vector\n",
        "max_features = None # how many unique words to use (i.e num rows in embedding vector)\n",
        "maxlen = 72 # max number of words in a question to use #99.99%\n",
        "\n",
        "## fill up the missing values\n",
        "X = train[\"comment_text\"].fillna(\"_na_\").values\n",
        "X_test = test[\"content\"].fillna(\"_na_\").values\n",
        "\n",
        "## Tokenize the sentences\n",
        "tokenizer = Tokenizer(num_words=max_features, filters='')\n",
        "tokenizer.fit_on_texts(list(X))\n",
        "\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "## Pad the sentences \n",
        "X = pad_sequences(X, maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "\n",
        "## Get the target values\n",
        "Y = train['toxic'].values\n",
        "\n",
        "sub = test[['id']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDwpxEJc5ex5",
        "colab_type": "code",
        "outputId": "bc880398-edc6-41c1-8d07-9c5bc142717f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "del train, test\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBsIFBK07I_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "max_features = len(word_index)+1\n",
        "def load_glove(word_index):\n",
        "    EMBEDDING_FILE = 'quora/glove.840B.300d/glove.840B.300d.txt'\n",
        "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if o.split(\" \")[0] in word_index)\n",
        "\n",
        "    all_embs = np.stack(embeddings_index.values())\n",
        "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "    embed_size = all_embs.shape[1]\n",
        "\n",
        "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= max_features: continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "    return embedding_matrix \n",
        "    \n",
        "def load_fasttext(word_index):    \n",
        "    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
        "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100 and o.split(\" \")[0] in word_index )\n",
        "\n",
        "    all_embs = np.stack(embeddings_index.values())\n",
        "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "    embed_size = all_embs.shape[1]\n",
        "\n",
        "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= max_features: continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "def load_para(word_index):\n",
        "    EMBEDDING_FILE = '/kaggle/input/quora-insincere-questions-classification/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
        "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100 and o.split(\" \")[0] in word_index)\n",
        "\n",
        "    all_embs = np.stack(embeddings_index.values())\n",
        "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "    embed_size = all_embs.shape[1]\n",
        "    \n",
        "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= max_features: continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
        "    \n",
        "    return embedding_matrix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7nw_FzW7QPr",
        "colab_type": "code",
        "outputId": "82dcbc10-380b-422d-c99a-4c7688a31889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "embedding_matrix_1 = load_glove(word_index)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxeC5Gxq7QcU",
        "colab_type": "code",
        "outputId": "61228b74-477e-4198-c97d-49239d987192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " data\t\t\t\t\t\t    sample_submission.csv.zip\n",
            " embeddings.zip\t\t\t\t\t    submission.csv\n",
            " Inference.ipynb\t\t\t\t    submission_nb.csv\n",
            " jigsaw-train-multilingual-coments-google-api.zip   test.csv.zip\n",
            "'Kaggle Data Load script.ipynb'\t\t\t    train.csv.zip\n",
            " NB.ipynb\t\t\t\t\t    Untitled0.ipynb\n",
            " quora\t\t\t\t\t\t   'zip archive'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nVVS4Mk7QfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkdqVaSs7QkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}